{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/frederickivens/Documents/Codes/fi246/demand/historic_demand_data.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/frederickivens/Documents/Codes/fi246/demand/historic_demand_data.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/frederickivens/Documents/Codes/fi246/demand/historic_demand_data.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/frederickivens/Documents/Codes/fi246/demand/historic_demand_data.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdictionaries\u001b[39;00m \u001b[39mimport\u001b[39;00m regions_dic\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from ..dictionaries import regions_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hourly_demand_data(file_path, sheet_name, year):\n",
    "    # Load the data from the specified sheet\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Convert DateUTC to datetime\n",
    "    df['DateUTC'] = pd.to_datetime(df['DateUTC'])\n",
    "\n",
    "    # Get unique country codes\n",
    "    unique_countries = df['CountryCode'].unique()\n",
    "    \n",
    "    # Create a dictionary to store the time series data for each country\n",
    "    hourly_demand_dict = {}\n",
    "\n",
    "    # Iterate through each country and store the data\n",
    "    for country in unique_countries:\n",
    "        country_data = df[df['CountryCode'] == country].set_index('DateUTC')['Value']\n",
    "\n",
    "        # Remove duplicates by taking the first occurrence\n",
    "        country_data = country_data[~country_data.index.duplicated(keep='first')]\n",
    "        \n",
    "        # Generate a full date range for the year 2021\n",
    "        full_date_range = pd.date_range(start=f'{year}-01-01', end=f'{year}-12-31 23:00:00', freq='h')\n",
    "        \n",
    "        # Reindex the country data to include the full date range, filling missing values with NaN\n",
    "        country_data = country_data.reindex(full_date_range, fill_value=np.nan)\n",
    "        \n",
    "        # Store the time series data in the dictionary\n",
    "        hourly_demand_dict[country] = country_data\n",
    "\n",
    "    return hourly_demand_dict\n",
    "\n",
    "def missing_countries(dict):\n",
    "    for country in dict.keys():\n",
    "        if dict[country].count() < 8760:\n",
    "            print(country, dict[country].count())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/frederickivens/Documents/Codes/fi246/demand/historic_demand_data.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/frederickivens/Documents/Codes/fi246/demand/historic_demand_data.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dic\u001b[39m.\u001b[39mregions_dic[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dic' is not defined"
     ]
    }
   ],
   "source": [
    "dic.regions_dic[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_dict = {}\n",
    "# 2015-217\n",
    "file_path = 'MHLV_data-2015-2019.xlsx'\n",
    "sheet_name = '2015-2017'\n",
    "# Load the hourly demand data\n",
    "years = range(2015, 2018)\n",
    "for year in years:\n",
    "    demand_dict[year] = load_hourly_demand_data(file_path, sheet_name, year)\n",
    "# 2018-2019\n",
    "file_path = 'MHLV_data-2015-2019.xlsx'\n",
    "sheet_name = '2018-2019'\n",
    "years = range(2018, 2020)\n",
    "for year in years:\n",
    "    demand_dict[year] = load_hourly_demand_data(file_path, sheet_name, year)\n",
    "# 2021\n",
    "file_path = 'monthly_hourly_load_values_2021.xlsx'\n",
    "sheet_name = 'monthly_hourly_load_values_2021'\n",
    "demand_dict[2021] = load_hourly_demand_data(file_path, sheet_name, 2021)\n",
    "# 2022\n",
    "file_path = 'monthly_hourly_load_values_2022.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "demand_dict[2022] = load_hourly_demand_data(file_path, sheet_name, 2022)\n",
    "# 2023\n",
    "file_path = 'monthly_hourly_load_values_2023.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "demand_dict[2023] = load_hourly_demand_data(file_path, sheet_name, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     4343.000000\n",
       "mean     38534.224301\n",
       "std       8391.912344\n",
       "min      21574.830000\n",
       "25%      31876.665000\n",
       "50%      37706.400000\n",
       "75%      44158.350000\n",
       "max      60573.860000\n",
       "Name: Value, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demand_dict[2019]['GB'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 ['AL', 'AT', 'BA', 'BE', 'BG', 'CH', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GB', 'GR', 'HR', 'HU', 'IE', 'IS', 'IT', 'LT', 'LU', 'LV', 'ME', 'MK', 'NL', 'NO', 'PL', 'PT', 'RO', 'RS', 'SE', 'SI', 'SK', 'TR']\n",
      "2016 ['AL', 'AT', 'BA', 'BE', 'BG', 'CH', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GB', 'GR', 'HR', 'HU', 'IE', 'IS', 'IT', 'LT', 'LU', 'LV', 'ME', 'MK', 'NL', 'NO', 'PL', 'PT', 'RO', 'RS', 'SE', 'SI', 'SK', 'TR']\n",
      "2017 ['AL', 'AT', 'BA', 'BE', 'BG', 'CH', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GB', 'GR', 'HR', 'HU', 'IE', 'IS', 'IT', 'LT', 'LU', 'LV', 'ME', 'MK', 'NL', 'NO', 'PL', 'PT', 'RO', 'RS', 'SE', 'SI', 'SK', 'TR']\n",
      "2018 ['AL', 'AT', 'BA', 'BE', 'BG', 'CH', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GB', 'GR', 'HR', 'HU', 'IE', 'IS', 'IT', 'LT', 'LU', 'LV', 'ME', 'MK', 'NL', 'NO', 'PL', 'PT', 'RO', 'RS', 'SE', 'SI', 'SK', 'TR']\n",
      "2019 ['AL', 'AT', 'BA', 'BE', 'BG', 'CH', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GB', 'GR', 'HR', 'HU', 'IE', 'IS', 'IT', 'LT', 'LU', 'LV', 'ME', 'MK', 'NL', 'NO', 'PL', 'PT', 'RO', 'RS', 'SE', 'SI', 'SK', 'TR']\n",
      "2021 ['AL', 'AT', 'BA', 'BE', 'BG', 'CH', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GB', 'GE', 'GR', 'HR', 'HU', 'IE', 'IT', 'LT', 'LU', 'LV', 'MD', 'ME', 'MK', 'NL', 'NO', 'PL', 'PT', 'RO', 'RS', 'SE', 'SI', 'SK', 'UA', 'XK']\n",
      "2022 ['AL', 'AT', 'BA', 'BE', 'BG', 'CH', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GE', 'GR', 'HR', 'HU', 'IE', 'IT', 'LT', 'LU', 'LV', 'MD', 'ME', 'MK', 'NL', 'NO', 'PL', 'PT', 'RO', 'RS', 'SE', 'SI', 'SK', 'UA', 'XK']\n",
      "2023 ['AL', 'AT', 'BA', 'BE', 'BG', 'CH', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GB', 'GE', 'GR', 'HR', 'HU', 'IE', 'IT', 'LT', 'LU', 'LV', 'MD', 'ME', 'MK', 'NL', 'NO', 'PL', 'PT', 'RO', 'RS', 'SE', 'SI', 'SK', 'XK']\n",
      "2015\n",
      "BG 2\n",
      "EE 2\n",
      "FI 2\n",
      "GR 2\n",
      "LT 2\n",
      "LV 2\n",
      "RO 2\n",
      "TR 2\n",
      "AT 1\n",
      "BA 1\n",
      "BE 1\n",
      "CH 1\n",
      "CY 1\n",
      "CZ 1\n",
      "DK 1\n",
      "ES 1\n",
      "FR 1\n",
      "HR 1\n",
      "HU 1\n",
      "IT 1\n",
      "LU 1\n",
      "ME 1\n",
      "MK 1\n",
      "NL 1\n",
      "NO 1\n",
      "PL 1\n",
      "RS 1\n",
      "SE 1\n",
      "SI 1\n",
      "SK 1\n",
      "GB 0\n",
      "IE 0\n",
      "IS 0\n",
      "PT 0\n",
      "AL 0\n",
      "2016\n",
      "AL 1\n",
      "2017\n",
      "2018\n",
      "AL 8759\n",
      "CY 7724\n",
      "EE 8757\n",
      "GR 8758\n",
      "LU 8759\n",
      "SE 8759\n",
      "2019\n",
      "AL 0\n",
      "AT 5830\n",
      "BA 6550\n",
      "BE 5830\n",
      "BG 5829\n",
      "CH 6550\n",
      "CY 0\n",
      "CZ 7295\n",
      "DE 5830\n",
      "DK 7295\n",
      "EE 0\n",
      "ES 7295\n",
      "FI 5085\n",
      "FR 4342\n",
      "GB 4343\n",
      "GR 0\n",
      "HR 7295\n",
      "HU 7295\n",
      "IE 0\n",
      "IS 5088\n",
      "IT 4342\n",
      "LT 6549\n",
      "LU 0\n",
      "LV 5829\n",
      "ME 1415\n",
      "MK 6924\n",
      "NL 2878\n",
      "NO 3622\n",
      "PL 4342\n",
      "PT 7296\n",
      "RO 5829\n",
      "RS 4342\n",
      "SE 0\n",
      "SI 3622\n",
      "SK 6550\n",
      "TR 5085\n",
      "2021\n",
      "AL 1632\n",
      "AT 8759\n",
      "BA 8693\n",
      "BE 8759\n",
      "BG 8759\n",
      "CH 8759\n",
      "CY 8578\n",
      "CZ 8759\n",
      "DE 8759\n",
      "DK 8759\n",
      "EE 8755\n",
      "ES 8758\n",
      "FI 8759\n",
      "FR 8750\n",
      "GB 5052\n",
      "GR 8757\n",
      "GE 179\n",
      "HR 8759\n",
      "HU 8759\n",
      "IE 8598\n",
      "IT 8759\n",
      "LT 8759\n",
      "LU 8759\n",
      "LV 8758\n",
      "MD 8583\n",
      "ME 8759\n",
      "MK 8615\n",
      "NL 8759\n",
      "NO 8758\n",
      "PL 8759\n",
      "PT 8759\n",
      "RO 8680\n",
      "RS 8759\n",
      "SE 8759\n",
      "SI 8759\n",
      "SK 8759\n",
      "UA 8741\n",
      "XK 3188\n",
      "2022\n",
      "AL 6360\n",
      "AT 8759\n",
      "BA 4366\n",
      "BE 8759\n",
      "BG 8759\n",
      "CH 8759\n",
      "CY 3809\n",
      "CZ 8758\n",
      "DE 8759\n",
      "DK 8759\n",
      "EE 8758\n",
      "ES 8759\n",
      "FI 8759\n",
      "FR 8729\n",
      "GE 8740\n",
      "GR 8758\n",
      "HR 8759\n",
      "HU 8759\n",
      "IE 3437\n",
      "IT 8759\n",
      "LT 8757\n",
      "LU 8759\n",
      "LV 8759\n",
      "MD 8505\n",
      "ME 8759\n",
      "MK 8759\n",
      "NL 8759\n",
      "NO 8759\n",
      "PL 8759\n",
      "PT 8759\n",
      "RO 8758\n",
      "RS 8757\n",
      "SE 8759\n",
      "SI 8754\n",
      "SK 8758\n",
      "UA 1333\n",
      "XK 8759\n",
      "2023\n",
      "AL 8614\n",
      "AT 8759\n",
      "BA 8742\n",
      "BE 8759\n",
      "BG 8759\n",
      "CH 8758\n",
      "CZ 8756\n",
      "DE 8759\n",
      "DK 8758\n",
      "EE 8756\n",
      "ES 8759\n",
      "FI 8759\n",
      "FR 8754\n",
      "GB 1282\n",
      "GE 8562\n",
      "GR 8759\n",
      "HR 8759\n",
      "HU 8757\n",
      "IE 4359\n",
      "IT 8759\n",
      "LT 8759\n",
      "LU 8759\n",
      "LV 8759\n",
      "MD 8708\n",
      "ME 8759\n",
      "MK 8759\n",
      "NL 8759\n",
      "NO 8759\n",
      "PL 8759\n",
      "PT 8759\n",
      "RO 8759\n",
      "RS 8614\n",
      "SE 8759\n",
      "SI 8745\n",
      "SK 8758\n",
      "XK 8759\n"
     ]
    }
   ],
   "source": [
    "for year in demand_dict:\n",
    "    print(year, sorted(demand_dict[year].keys()))\n",
    "for year in demand_dict:\n",
    "    print(year)\n",
    "    missing_countries(demand_dict[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_demand_dic = demand_dict[2017].copy()\n",
    "combined_demand_dic['XK'] = demand_dict[2023]['XK'].copy()\n",
    "combined_demand_dic['UA'] = demand_dict[2021]['UA'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = {\n",
    "    'AL': 'Albania',\n",
    "    'AT': 'Austria',\n",
    "    'BA': 'Bosnia and Herzegovina',\n",
    "    'BE': 'Belgium',\n",
    "    'BG': 'Bulgaria',\n",
    "    'CH': 'Switzerland',\n",
    "    'CY': 'Cyprus',\n",
    "    'CZ': 'Czech Republic',\n",
    "    'DE': 'Germany',\n",
    "    'DK': 'Denmark',\n",
    "    'EE': 'Estonia',\n",
    "    'ES': 'Spain',\n",
    "    'FI': 'Finland',\n",
    "    'FR': 'France',\n",
    "    'GB': 'United Kingdom',\n",
    "    'GR': 'Greece',\n",
    "    'HR': 'Croatia',\n",
    "    'HU': 'Hungary',\n",
    "    'IE': 'Ireland',\n",
    "    'IS': 'Iceland',  # Added Iceland\n",
    "    'IT': 'Italy',\n",
    "    'LT': 'Lithuania',\n",
    "    'LU': 'Luxembourg',\n",
    "    'LV': 'Latvia',\n",
    "    'ME': 'Montenegro',\n",
    "    'MK': 'North Macedonia',\n",
    "    'NL': 'Netherlands',\n",
    "    'NO': 'Norway',\n",
    "    'PL': 'Poland',\n",
    "    'PT': 'Portugal',\n",
    "    'RO': 'Romania',\n",
    "    'RS': 'Serbia',\n",
    "    'SE': 'Sweden',\n",
    "    'SI': 'Slovenia',\n",
    "    'SK': 'Slovakia',\n",
    "    'TR': 'Turkey',\n",
    "    'UA': 'Ukraine',\n",
    "    'XK': 'Kosovo'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dic = {}\n",
    "for country_code in country_codes:\n",
    "    new_dic[country_codes[country_code]] = combined_demand_dic[country_code].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where you want to save the CSV files\n",
    "output_dir = 'Europe 2023'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Concatenate all country dataframes for the year into a single dataframe\n",
    "combined_df = pd.concat(new_dic.values(), keys=new_dic.keys(), names=['Country', 'Date'])\n",
    "\n",
    "# Define the file name and path\n",
    "file_name = 'europe_historic_demand_2023.csv'\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "# Export the combined DataFrame to a CSV file\n",
    "combined_df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(regions_dic[9][\"countries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE ONLY COUNTRY MISSING IS MALTA\n",
    "europe_countries_dic = {}\n",
    "counter = 0\n",
    "for country in regions_dic[9][\"countries\"]:\n",
    "    if country in new_dic.keys():\n",
    "        #print(f'{country} in list')\n",
    "        europe_countries_dic[country] = new_dic[country].copy()\n",
    "    else:\n",
    "        #print(f'{country} not in list')\n",
    "        counter += 1\n",
    "#print(f'{counter} countries not in list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where you want to save the CSV files\n",
    "def save_dictionary(output_dir, dataframe, name):\n",
    "    output_dir = 'Europe 2023'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Concatenate all country dataframes for the year into a single dataframe\n",
    "    combined_df = pd.concat(dataframe.values(), keys=dataframe.keys(), names=['Country', 'Date'])\n",
    "\n",
    "    # Define the file name and path\n",
    "    file_name = f'{name}.csv'\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Export the combined DataFrame to a CSV file\n",
    "    combined_df.to_csv(file_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dictionary('Europe 2023', europe_countries_dic, 'europe_2017_demand_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Total_Demand\n",
      "2017-01-01 00:00:00     343752.82\n",
      "2017-01-01 01:00:00     331256.25\n",
      "2017-01-01 02:00:00     317225.61\n",
      "2017-01-01 03:00:00     307590.34\n",
      "2017-01-01 04:00:00     304256.15\n"
     ]
    }
   ],
   "source": [
    "# Convert the dictionary of DataFrames into a single DataFrame\n",
    "europe_total_df = pd.DataFrame(europe_countries_dic)\n",
    "\n",
    "# Sum the hourly demand across all countries\n",
    "europe_total_df['Total_Demand'] = europe_total_df.sum(axis=1)\n",
    "\n",
    "# Keep only the Total_Demand column\n",
    "total_demand_df = europe_total_df[['Total_Demand']]\n",
    "\n",
    "# Display the first few rows of the DataFrame with the total demand\n",
    "print(total_demand_df.head())\n",
    "\n",
    "# Optionally, save the DataFrame with only total demand to a CSV file\n",
    "total_demand_df.to_csv('total_hourly_demand_europe_node_9.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# North America"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  UTC Time at End of Hour  Demand (MW)\n",
      "0     2017-01-01 06:00:00     218576.0\n",
      "1     2017-01-01 07:00:00     309560.0\n",
      "2     2017-01-01 08:00:00     326770.0\n",
      "3     2017-01-01 09:00:00     374644.0\n",
      "4     2017-01-01 10:00:00     370568.0\n",
      "Data saved to total_hourly_demand_usa_2017.csv\n"
     ]
    }
   ],
   "source": [
    "def load_and_process_data(file_path):\n",
    "    # Load the data from the CSV file as strings to handle commas\n",
    "    df = pd.read_csv(file_path, dtype=str, low_memory=False)\n",
    "\n",
    "    # Remove commas from 'Demand (MW)' column and convert to float\n",
    "    df['Demand (MW)'] = df['Demand (MW)'].str.replace(',', '').astype(float)\n",
    "\n",
    "    # Convert 'UTC Time at End of Hour' to datetime\n",
    "    df['UTC Time at End of Hour'] = pd.to_datetime(df['UTC Time at End of Hour'])\n",
    "\n",
    "    # Check for duplicates based on 'UTC Time at End of Hour' and 'Balancing Authority'\n",
    "    duplicates = df.duplicated(subset=['UTC Time at End of Hour', 'Balancing Authority'], keep=False)\n",
    "    \n",
    "    # Print duplicates if any\n",
    "    if duplicates.any():\n",
    "        print(\"Found duplicates:\")\n",
    "        print(df[duplicates])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['UTC Time at End of Hour', 'Balancing Authority'])\n",
    "\n",
    "    # Aggregate the hourly demand for each balancing authority\n",
    "    df = df.groupby(['UTC Time at End of Hour', 'Balancing Authority'])['Demand (MW)'].sum().reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load data for both halves of the year\n",
    "file_path_1 = 'EIA930_BALANCE_2017_Jan_Jun.csv'  # Replace with actual path to the first file\n",
    "file_path_2 = 'EIA930_BALANCE_2017_Jul_Dec.csv'  # Replace with actual path to the second file\n",
    "\n",
    "df_1 = load_and_process_data(file_path_1)\n",
    "df_2 = load_and_process_data(file_path_2)\n",
    "\n",
    "# Combine the data from both halves of the year\n",
    "combined_df = pd.concat([df_1, df_2])\n",
    "\n",
    "# Sum the hourly demand across all balancing authorities\n",
    "total_demand_df = combined_df.groupby('UTC Time at End of Hour')['Demand (MW)'].sum().reset_index()\n",
    "\n",
    "# Display the first few rows of the total demand DataFrame\n",
    "print(total_demand_df.head())\n",
    "\n",
    "# Save the total demand DataFrame to a CSV file\n",
    "total_demand_df.to_csv('total_hourly_demand_usa_2017.csv', index=False)\n",
    "\n",
    "print(\"Data saved to total_hourly_demand_usa_2017.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def load_and_process_data(file_path):\n",
    "    try:\n",
    "        # Load the data from the CSV file\n",
    "        df = pd.read_csv(file_path, skiprows=3, on_bad_lines='skip')\n",
    "\n",
    "        # Convert 'DATE' to datetime\n",
    "        df['DATE'] = pd.to_datetime(df['DATE'], format='%Y-%m-%d')\n",
    "\n",
    "        # Combine 'DATE' and 'HOUR' to create a datetime index\n",
    "        df['DateTime'] = df.apply(lambda row: row['DATE'] + pd.Timedelta(hours=row['HOUR']-1), axis=1)\n",
    "\n",
    "        # Set 'DateTime' as the index\n",
    "        df.set_index('DateTime', inplace=True)\n",
    "\n",
    "        # Select relevant columns and sum the hourly demand for Ontario\n",
    "        total_demand_df = df[['TOTAL_CONSUMPTION']].rename(columns={'TOTAL_CONSUMPTION': 'Total_Demand'})\n",
    "\n",
    "        return total_demand_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# List all monthly CSV files\n",
    "file_paths = glob.glob('/Users/frederickivens/Documents/Codes/fi246/demand/Histroric_raw_data/Canada/*.csv')  # Updated file path\n",
    "\n",
    "# Check if any files were found\n",
    "if not file_paths:\n",
    "    print(\"No files found. Please check the file path.\")\n",
    "else:\n",
    "    print(f\"Found {len(file_paths)} files.\")\n",
    "\n",
    "# Load and process data for each month\n",
    "monthly_dfs = [load_and_process_data(file_path) for file_path in file_paths]\n",
    "\n",
    "# Filter out None values (files that failed to load)\n",
    "monthly_dfs = [df for df in monthly_dfs if df is not None]\n",
    "\n",
    "# Check if any dataframes were loaded successfully\n",
    "if not monthly_dfs:\n",
    "    print(\"No valid dataframes to concatenate. Please check the input files.\")\n",
    "else:\n",
    "    # Combine the data from all months\n",
    "    combined_df = pd.concat(monthly_dfs)\n",
    "\n",
    "    # Check for duplicates\n",
    "    duplicates = combined_df.duplicated(keep=False)\n",
    "\n",
    "    # Print duplicates if any\n",
    "    if duplicates.any():\n",
    "        print(\"Found duplicates:\")\n",
    "        print(combined_df[duplicates])\n",
    "\n",
    "    # Remove duplicates\n",
    "    combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "    # Display the first few rows of the total demand DataFrame\n",
    "    print(combined_df.head())\n",
    "\n",
    "    # Save the combined DataFrame with total demand to a CSV file\n",
    "    combined_df.to_csv('total_hourly_demand_ontario_2017.csv')\n",
    "\n",
    "    print(\"Data saved to total_hourly_demand_ontario_2017.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ../Historic_raw_data/Canada/Alberta/Hourly-System-Load-Data-2020.xlsx\n",
      "Processing failed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path relative to the current working directory\n",
    "file_path = \"../Historic_raw_data/Canada/Alberta/Hourly-System-Load-Data-2020.xlsx\"\n",
    "\n",
    "# Function to read and process the Excel file\n",
    "def process_alberta_file(filepath):\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(filepath)\n",
    "    \n",
    "    # Combine DATE and HOUR ENDING into a single datetime column\n",
    "    df['DATETIME'] = pd.to_datetime(df['DATE'].astype(str) + ' ' + df['HOUR ENDING'].astype(str) + ':00', format='%m/%d/%Y %H:%M')\n",
    "    \n",
    "    # Set the DATETIME column as the index\n",
    "    df.set_index('DATETIME', inplace=True)\n",
    "    \n",
    "    # Select the SYSTEM LOAD column and resample to hourly intervals\n",
    "    df = df['SYSTEM LOAD'].resample('H').mean()\n",
    "    \n",
    "    # Convert the time to UTC (assuming the data is in local time, if known)\n",
    "    df.index = df.index.tz_localize('America/Edmonton', ambiguous='NaT', nonexistent='shift_forward').tz_convert('UTC')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process the file\n",
    "alberta_demand = process_alberta_file(file_path)\n",
    "\n",
    "if alberta_demand is not None:\n",
    "    # Save the processed data to a CSV file\n",
    "    alberta_demand.to_csv('processed_alberta_hourly_demand_2020.csv')\n",
    "    print(\"File processed and data saved to processed_alberta_hourly_demand_2020.csv.\")\n",
    "else:\n",
    "    print(\"Processing failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Base URL structure\n",
    "base_url = \"https://www.aemo.com.au/aemo/data/nem/priceanddemand/PRICE_AND_DEMAND_{year}{month}_{region}1.csv\"\n",
    "\n",
    "# Define regions and months for 2023\n",
    "regions = [\"NSW\", \"QLD\", \"SA\", \"TAS\", \"VIC\"]\n",
    "months = {\n",
    "    \"01\": \"January\",\n",
    "    \"02\": \"February\",\n",
    "    \"03\": \"March\",\n",
    "    \"04\": \"April\",\n",
    "    \"05\": \"May\",\n",
    "    \"06\": \"June\",\n",
    "    \"07\": \"July\",\n",
    "    \"08\": \"August\",\n",
    "    \"09\": \"September\",\n",
    "    \"10\": \"October\",\n",
    "    \"11\": \"November\",\n",
    "    \"12\": \"December\"\n",
    "}\n",
    "\n",
    "# Directory to save downloaded files\n",
    "download_dir = \"aemo_data_2023\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Function to download a file from a URL\n",
    "def download_file(url, filepath):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        print(f\"Requesting URL: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        with open(filepath, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded and saved to {filepath}\")\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Other error occurred: {err}\")\n",
    "\n",
    "# Iterate through regions and months to construct URLs and download data\n",
    "for region in regions:\n",
    "    for month_num, month_name in months.items():\n",
    "        # Construct the download URL\n",
    "        download_url = base_url.format(year=\"2023\", month=month_num, region=region)\n",
    "        \n",
    "        # Define the local file path to save the downloaded file\n",
    "        filepath = os.path.join(download_dir, f\"PRICE_AND_DEMAND_2023{month_num}_{region}1.csv\")\n",
    "        \n",
    "        # Download the file\n",
    "        print(f\"Downloading {download_url}...\")\n",
    "        download_file(download_url, filepath)\n",
    "\n",
    "print(\"All downloads completed.\")\n",
    "\n",
    "# Define regions and months for 2023\n",
    "regions = [\"NSW\", \"QLD\", \"SA\", \"TAS\", \"VIC\"]\n",
    "months = [\"{:02d}\".format(i) for i in range(1, 13)]\n",
    "\n",
    "# Directory containing downloaded files\n",
    "download_dir = \"aemo_data_2023\"\n",
    "\n",
    "# Function to read and process a single CSV file\n",
    "def process_file(filepath):\n",
    "    df = pd.read_csv(filepath, usecols=['SETTLEMENTDATE', 'TOTALDEMAND'])  # Only read necessary columns\n",
    "    df['SETTLEMENTDATE'] = pd.to_datetime(df['SETTLEMENTDATE'], format='%Y/%m/%d %H:%M:%S')\n",
    "    df['SETTLEMENTDATE'] = df['SETTLEMENTDATE'].dt.tz_localize('Australia/Sydney', ambiguous='NaT', nonexistent='shift_forward').dt.tz_convert('UTC')\n",
    "    df.set_index('SETTLEMENTDATE', inplace=True)\n",
    "    df = df.resample('h').mean()  # Resample to hourly data\n",
    "    return df['TOTALDEMAND']\n",
    "\n",
    "# Initialize a list to hold the demand data from all files\n",
    "all_demand = []\n",
    "\n",
    "# Iterate through regions and months to process files\n",
    "for region in regions:\n",
    "    for month in months:\n",
    "        # Define the local file path\n",
    "        filepath = os.path.join(download_dir, f\"PRICE_AND_DEMAND_2023{month}_{region}1.csv\")\n",
    "        \n",
    "        # Process the file if it exists\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Processing {filepath}...\")\n",
    "            monthly_demand = process_file(filepath)\n",
    "            all_demand.append(monthly_demand)\n",
    "\n",
    "# Combine all data\n",
    "combined_demand = pd.concat(all_demand).groupby(level=0).sum()\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "combined_demand.to_csv('combined_hourly_demand_2023.csv')\n",
    "\n",
    "print(\"All files processed and combined data saved to combined_hourly_demand_2023.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mphil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
